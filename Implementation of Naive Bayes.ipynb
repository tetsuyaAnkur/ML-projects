{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word is not in my vocablary\n",
      "['stupid', 'garbage', 'product'] classified as:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tkinter as tk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "class MyApp(tk.Tk):\n",
    "    def __init__(self):\n",
    "        tk.Tk.__init__(self)\n",
    "        self.entry = tk.Entry(self)\n",
    "        self.entry.pack()\n",
    "        close_button = tk.Button(self, text=\"Close\", command=self.close)\n",
    "        close_button.pack()\n",
    "        self.string = \"\"\n",
    "\n",
    "    def close(self):\n",
    "        global result\n",
    "        self.string = self.entry.get()\n",
    "        self.destroy()\n",
    "\n",
    "    def mainloop(self):\n",
    "        tk.Tk.mainloop(self)\n",
    "        return self.string\n",
    "stop=stopwords.words('english')\n",
    "\n",
    "for i in range(len(stop)):\n",
    "    stop.append(stop[i][0].upper() + stop[i][1:])\n",
    "for i in range(len(stop)):\n",
    "    stop.append(stop[i].upper())\n",
    "#print(stop)\n",
    "tknzr = TweetTokenizer()\n",
    "file= open('/home/harsh/amazon_cells_labelled.txt', 'r') \n",
    "c=file.readlines()\n",
    "#print(c)\n",
    "list_of_posts=[]\n",
    "list_of_classes=[]\n",
    "filter_words=[]\n",
    "filtered_words=[]\n",
    "for i in range(len(c)):\n",
    "    b=tknzr.tokenize(c[i])\n",
    "    list_of_posts.append(b)\n",
    "    list_of_classes.append(b[len(b)-1])\n",
    "    filter_words=[word for word in b if word not in stop]\n",
    "    filtered_words.append(filter_words)\n",
    "#print(filtered_words)\n",
    "list_class=[int(i) for i in list_of_classes]\n",
    "    #print(filter_words)\n",
    "    #print(list_of_posts)\n",
    "    #print(list_of_classes)\n",
    "#print(c.split())\n",
    "\n",
    "\n",
    "def loadDataSet():\n",
    "    postinglist = filtered_words\n",
    "    classVec = list_class\n",
    "    return postinglist,classVec\n",
    "\n",
    "def createVocabList(dataset):\n",
    "    vocabset = set([])\n",
    "    for document in dataset:\n",
    "        vocabset = vocabset | set(document)\n",
    "    return list(vocabset)\n",
    "#print(vocablist)\n",
    "def setOfWords2Vec(vocablist, inputset):\n",
    "    returnVec = [0]*len(vocablist)\n",
    "    for word in inputset:\n",
    "        if word in vocablist:\n",
    "            returnVec[vocablist.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word is not in my vocablary\")\n",
    "    return returnVec\n",
    "\n",
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "#print(myVocabList)\n",
    "#print(setOfWords2Vec(myVocabList,listOPosts[3] ))\n",
    "trainMatrix=[]\n",
    "for postinDoc in listOPosts:\n",
    "        trainMatrix.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "#print(trainMatrix)\n",
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    #print(trainMatrix)\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = np.zeros(numWords);\n",
    "    p1Num = np.zeros(numWords);\n",
    "    p0Denom = 0.0;\n",
    "    p1Denom = 0.0;\n",
    "    #print\n",
    "    \n",
    "    #print(trainMatrix[0])\n",
    "    #print(trainMatrix[0])\n",
    "    for i in range(numTrainDocs):\n",
    "        \n",
    "        #print(p1Num)\n",
    "        if(trainCategory[i] == 1):\n",
    "            p1Num = np.add(p1Num,trainMatrix[i]);\n",
    "            p1Denom += sum(trainMatrix[i]);\n",
    "        else:\n",
    "            p0Num = np.add(p0Num,trainMatrix[i]);\n",
    "            p0Denom += sum(trainMatrix[i]);\n",
    "    p1vect = [(i/p1Denom) for i in p1Num]\n",
    "    p0vect = [(i/p0Denom) for i in p0Num]\n",
    "    return p0vect,p1vect,pAbusive\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = np.dot(vec2Classify,p1Vec) + math.log(pClass1)         \n",
    "    p0 = np.dot(vec2Classify,p0Vec) + math.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "app = MyApp()\n",
    "result = app.mainloop()\n",
    "result=result.split()\n",
    "p0V,p1V,pAb = trainNB0(trainMatrix,listClasses)\n",
    "test_entry=result   \n",
    "thisDoc = setOfWords2Vec(myVocabList, test_entry)\n",
    "#print(len(p1V))\n",
    "#print(len(thisDoc))\n",
    "print(test_entry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
